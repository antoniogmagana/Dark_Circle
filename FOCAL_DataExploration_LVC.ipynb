{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8d8c52",
   "metadata": {},
   "source": [
    "# MOD_vehicle Data Exploration for LVC Toolkit Integration\n",
    "\n",
    "This notebook explores the MOD_vehicle dataset to understand its structure and potential applications in the LVC (Live, Virtual, Constructive) Toolkit.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The MOD_vehicle data contains multiple vehicle type classifications for acoustic and seismic detection.\n",
    "\n",
    "### Sensor Types:\n",
    "- **AUD**: Audio/Acoustic sensors (stereo)\n",
    "- **EHZ**: Seismic vertical component\n",
    "- **ENE**: Seismic east component  \n",
    "- **ENN**: Seismic north component\n",
    "- **ENZ**: Seismic Z component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "import scipy.signal as signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Base path to data - server location\n",
    "BASE_PATH = Path('/home/lvc_toolkit/datasets')\n",
    "print(f\"Base path exists: {BASE_PATH.exists()}\")\n",
    "print(f\"Base path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638444a6",
   "metadata": {},
   "source": [
    "## 1. MOD_vehicle Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore MOD_vehicle structure\n",
    "vehicle_path = BASE_PATH / 'MOD_vehicle'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOD_vehicle types:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "vehicle_types = []\n",
    "for vehicle_dir in sorted(vehicle_path.iterdir()):\n",
    "    if vehicle_dir.is_dir() and not vehicle_dir.name.startswith('.'):\n",
    "        # Check for subdirectories (rs1, rs2, etc.)\n",
    "        subdirs = [d for d in vehicle_dir.iterdir() if d.is_dir()]\n",
    "        print(f\"\\n{vehicle_dir.name}:\")\n",
    "        print(f\"  Recording sessions: {len(subdirs)}\")\n",
    "        if subdirs:\n",
    "            sample_files = list(subdirs[0].glob('*.csv'))\n",
    "            print(f\"  Files per session: {[f.name for f in sample_files[:5]]}\")  # Show first 5\n",
    "        vehicle_types.append(vehicle_dir.name)\n",
    "\n",
    "print(f\"\\nTotal vehicle types: {len(vehicle_types)}\")\n",
    "print(f\"Vehicle types: {vehicle_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17081b16",
   "metadata": {},
   "source": [
    "## 2. Data Format Analysis\n",
    "\n",
    "Let's examine the format of the seismic and audio data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a138a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze a seismic file from vehicle data\n",
    "sample_vehicle = vehicle_path / 'mustang' / 'rs1' / 'ehz.csv'\n",
    "\n",
    "print(\"Seismic Data Format Analysis (MOD_vehicle):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Note: Seismic CSV format is [amplitude timestamp] with trailing space\n",
    "seismic_df = pd.read_csv(sample_vehicle, sep='\\s+', header=None, names=['value', 'timestamp'])\n",
    "print(f\"Shape: {seismic_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(seismic_df.head(10))\n",
    "print(f\"\\nData statistics:\")\n",
    "print(seismic_df.describe())\n",
    "print(f\"\\nSample rate: ~{1/seismic_df['timestamp'].diff().mean():.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e399c",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Before analyzing the signals, we need to address data quality issues:\n",
    "- **DC Offset Removal**: Remove mean value to center signals around zero\n",
    "- **Outlier Detection**: Identify and handle anomalous values\n",
    "- **Data Quality Checks**: Verify signal integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dc_offset(signal_data):\n",
    "    \"\"\"Remove DC offset by subtracting the mean\"\"\"\n",
    "    return signal_data - np.mean(signal_data)\n",
    "\n",
    "def detect_outliers(signal_data, threshold=5):\n",
    "    \"\"\"Detect outliers using z-score method\"\"\"\n",
    "    mean = np.mean(signal_data)\n",
    "    std = np.std(signal_data)\n",
    "    z_scores = np.abs((signal_data - mean) / std)\n",
    "    outlier_indices = np.where(z_scores > threshold)[0]\n",
    "    return outlier_indices\n",
    "\n",
    "def remove_outliers(signal_data, threshold=5, method='clip'):\n",
    "    \"\"\"Remove or clip outliers\n",
    "    \n",
    "    Parameters:\n",
    "    - method: 'clip' (replace with threshold) or 'interpolate' (linear interpolation)\n",
    "    \"\"\"\n",
    "    outlier_indices = detect_outliers(signal_data, threshold)\n",
    "    cleaned = signal_data.copy()\n",
    "    \n",
    "    if method == 'clip':\n",
    "        # Clip to +/- threshold standard deviations\n",
    "        mean = np.mean(signal_data)\n",
    "        std = np.std(signal_data)\n",
    "        cleaned = np.clip(cleaned, mean - threshold*std, mean + threshold*std)\n",
    "    elif method == 'interpolate' and len(outlier_indices) > 0:\n",
    "        # Linear interpolation for outliers\n",
    "        mask = np.ones(len(signal_data), dtype=bool)\n",
    "        mask[outlier_indices] = False\n",
    "        good_indices = np.where(mask)[0]\n",
    "        cleaned[outlier_indices] = np.interp(outlier_indices, good_indices, signal_data[good_indices])\n",
    "    \n",
    "    return cleaned, outlier_indices\n",
    "\n",
    "def check_data_quality(signal_data, signal_name=\"Signal\"):\n",
    "    \"\"\"Perform comprehensive data quality checks\"\"\"\n",
    "    print(f\"\\n{signal_name} Quality Check:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"  Samples: {len(signal_data)}\")\n",
    "    print(f\"  Mean (DC offset): {np.mean(signal_data):.4f}\")\n",
    "    print(f\"  Std Dev: {np.std(signal_data):.4f}\")\n",
    "    print(f\"  Min: {np.min(signal_data):.4f}\")\n",
    "    print(f\"  Max: {np.max(signal_data):.4f}\")\n",
    "    print(f\"  Range: {np.ptp(signal_data):.4f}\")\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    nan_count = np.sum(np.isnan(signal_data))\n",
    "    inf_count = np.sum(np.isinf(signal_data))\n",
    "    print(f\"  NaN values: {nan_count}\")\n",
    "    print(f\"  Inf values: {inf_count}\")\n",
    "    \n",
    "    # Outlier detection\n",
    "    outliers = detect_outliers(signal_data, threshold=5)\n",
    "    print(f\"  Outliers (>5σ): {len(outliers)} ({100*len(outliers)/len(signal_data):.2f}%)\")\n",
    "    \n",
    "    # Signal-to-noise ratio estimate (using RMS)\n",
    "    rms = np.sqrt(np.mean(signal_data**2))\n",
    "    noise_estimate = np.std(signal_data[:100])  # Assuming first 100 samples might be noise\n",
    "    if noise_estimate > 0:\n",
    "        snr = 20 * np.log10(rms / noise_estimate)\n",
    "        print(f\"  Estimated SNR: {snr:.2f} dB\")\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(signal_data),\n",
    "        'std': np.std(signal_data),\n",
    "        'outliers': len(outliers),\n",
    "        'nan_count': nan_count,\n",
    "        'inf_count': inf_count\n",
    "    }\n",
    "\n",
    "print(\"Data cleaning functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data cleaning on seismic data from MOD_vehicle\n",
    "print(\"RAW DATA ANALYSIS:\")\n",
    "raw_quality = check_data_quality(seismic_df['value'].values, \"Raw Seismic EHZ\")\n",
    "\n",
    "# Apply DC offset removal\n",
    "seismic_cleaned = remove_dc_offset(seismic_df['value'].values)\n",
    "print(\"\\n\\nAFTER DC OFFSET REMOVAL:\")\n",
    "dc_quality = check_data_quality(seismic_cleaned, \"DC-Corrected Seismic EHZ\")\n",
    "\n",
    "# Remove outliers\n",
    "seismic_cleaned_no_outliers, outlier_idx = remove_outliers(seismic_cleaned, threshold=5, method='clip')\n",
    "print(\"\\n\\nAFTER OUTLIER REMOVAL:\")\n",
    "final_quality = check_data_quality(seismic_cleaned_no_outliers, \"Fully Cleaned Seismic EHZ\")\n",
    "\n",
    "print(f\"\\n\\nCLEANING SUMMARY:\")\n",
    "print(f\"  DC offset removed: {raw_quality['mean']:.4f}\")\n",
    "print(f\"  Outliers detected: {len(outlier_idx)}\")\n",
    "print(f\"  Data quality improved: ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cleaning effects on MOD_vehicle data\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "time_rel = seismic_df['timestamp'] - seismic_df['timestamp'].iloc[0]\n",
    "\n",
    "# Raw data - Time series\n",
    "axes[0, 0].plot(time_rel, seismic_df['value'], linewidth=0.8, color='red', alpha=0.7)\n",
    "axes[0, 0].set_title('Raw Data - Time Series', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=np.mean(seismic_df['value']), color='black', linestyle='--', label=f'Mean: {np.mean(seismic_df[\"value\"]):.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Raw data - Histogram\n",
    "axes[0, 1].hist(seismic_df['value'], bins=100, color='red', alpha=0.6, edgecolor='black')\n",
    "axes[0, 1].set_title('Raw Data - Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Amplitude')\n",
    "axes[0, 1].axvline(x=np.mean(seismic_df['value']), color='black', linestyle='--', linewidth=2)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# DC corrected - Time series\n",
    "axes[1, 0].plot(time_rel, seismic_cleaned, linewidth=0.8, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('After DC Offset Removal - Time Series', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Amplitude')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='--', label='Mean: 0.00')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# DC corrected - Histogram\n",
    "axes[1, 1].hist(seismic_cleaned, bins=100, color='orange', alpha=0.6, edgecolor='black')\n",
    "axes[1, 1].set_title('After DC Offset Removal - Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Amplitude')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Fully cleaned - Time series\n",
    "axes[2, 0].plot(time_rel, seismic_cleaned_no_outliers, linewidth=0.8, color='green', alpha=0.7)\n",
    "axes[2, 0].set_title('After Outlier Removal - Time Series', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Time (seconds)')\n",
    "axes[2, 0].set_ylabel('Amplitude')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "# Fully cleaned - Histogram\n",
    "axes[2, 1].hist(seismic_cleaned_no_outliers, bins=100, color='green', alpha=0.6, edgecolor='black')\n",
    "axes[2, 1].set_title('After Outlier Removal - Distribution', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Amplitude')\n",
    "axes[2, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Data Cleaning Pipeline - Seismic EHZ (Mustang)', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b83d8b",
   "metadata": {},
   "source": [
    "## 4. Signal Visualization\n",
    "\n",
    "Analyze cleaned seismic data from vehicle recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6276208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cleaned seismic data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "time_rel = seismic_df['timestamp'] - seismic_df['timestamp'].iloc[0]\n",
    "\n",
    "# Time series\n",
    "axes[0].plot(time_rel, seismic_cleaned_no_outliers, linewidth=0.8, color='green')\n",
    "axes[0].set_title('Seismic EHZ - Mustang (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (seconds)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram\n",
    "axes[1].hist(seismic_cleaned_no_outliers, bins=100, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Amplitude Distribution (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Amplitude')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5, linewidth=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983b7a0",
   "metadata": {},
   "source": [
    "## 5. Frequency Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FFT for seismic data\n",
    "def compute_spectrum(signal_data, sample_rate):\n",
    "    \"\"\"Compute frequency spectrum using FFT\"\"\"\n",
    "    N = len(signal_data)\n",
    "    yf = fft(signal_data)\n",
    "    xf = fftfreq(N, 1/sample_rate)[:N//2]\n",
    "    power = 2.0/N * np.abs(yf[:N//2])\n",
    "    return xf, power\n",
    "\n",
    "# Compute spectrum for cleaned seismic data\n",
    "sample_rate = 1 / seismic_df['timestamp'].diff().mean()\n",
    "freq, power = compute_spectrum(seismic_cleaned_no_outliers, sample_rate)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "ax.semilogy(freq, power, linewidth=0.8, color='green', alpha=0.7)\n",
    "ax.set_title('Seismic Frequency Spectrum - Mustang (Cleaned)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Power (log scale)')\n",
    "ax.set_xlim([0, 50])  # Focus on typical seismic frequencies\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spectrogram for seismic data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "f, t, Sxx = signal.spectrogram(seismic_cleaned_no_outliers, sample_rate, nperseg=512)\n",
    "im = ax.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_title('Spectrogram - Seismic EHZ (Mustang, Cleaned)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 50])\n",
    "plt.colorbar(im, ax=ax, label='Power (dB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29890862",
   "metadata": {},
   "source": [
    "## 6. Multi-Sensor Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all seismic sensors for comparison (with cleaning)\n",
    "sensor_types = ['ehz', 'ene', 'enn', 'enz']\n",
    "seismic_data = {}\n",
    "seismic_data_cleaned = {}\n",
    "\n",
    "for sensor in sensor_types:\n",
    "    file_path = vehicle_path / 'mustang' / 'rs1' / f'{sensor}.csv'\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path, sep='\\s+', header=None, names=['value', 'timestamp'])\n",
    "        seismic_data[sensor] = df\n",
    "        \n",
    "        # Clean the data\n",
    "        cleaned = remove_dc_offset(df['value'].values)\n",
    "        cleaned, _ = remove_outliers(cleaned, threshold=5, method='clip')\n",
    "        seismic_data_cleaned[sensor] = cleaned\n",
    "        \n",
    "        print(f\"Loaded and cleaned {sensor.upper()}: {len(df)} samples\")\n",
    "\n",
    "print(f\"\\nLoaded {len(seismic_data)} seismic sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all seismic sensors (cleaned)\n",
    "fig, axes = plt.subplots(len(seismic_data), 1, figsize=(14, 12))\n",
    "\n",
    "colors = ['green', 'orange', 'purple', 'brown']\n",
    "\n",
    "for (sensor, df), cleaned, ax, color in zip(seismic_data.items(), seismic_data_cleaned.values(), axes, colors):\n",
    "    time_rel = df['timestamp'] - df['timestamp'].iloc[0]\n",
    "    ax.plot(time_rel, cleaned, linewidth=0.8, color=color, label=sensor.upper())\n",
    "    ax.set_title(f'Seismic Sensor: {sensor.upper()} (Cleaned)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)')\n",
    "fig.suptitle('Multi-Sensor Seismic Data Comparison - Mustang (Cleaned)', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f238ed4d",
   "metadata": {},
   "source": [
    "## 7. Vehicle Classification Analysis\n",
    "\n",
    "**Note:** All features extracted from cleaned data (DC offset removed, outliers clipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from different vehicle types (with cleaning)\n",
    "vehicle_features = []\n",
    "\n",
    "sample_vehicles = ['bicycle', 'motor', 'mustang', 'pickup', 'scooter', 'tesla', 'walk']\n",
    "\n",
    "for vehicle in sample_vehicles:\n",
    "    vehicle_dir = vehicle_path / vehicle\n",
    "    if vehicle_dir.exists():\n",
    "        # Get first recording session\n",
    "        sessions = [d for d in vehicle_dir.iterdir() if d.is_dir()]\n",
    "        if sessions:\n",
    "            ehz_file = sessions[0] / 'ehz.csv'\n",
    "            if ehz_file.exists():\n",
    "                # Read first 100000 samples for consistency\n",
    "                df = pd.read_csv(ehz_file, sep='\\s+', header=None, names=['value', 'timestamp'], nrows=100000)\n",
    "                \n",
    "                # Clean the data\n",
    "                cleaned_signal = remove_dc_offset(df['value'].values)\n",
    "                cleaned_signal, _ = remove_outliers(cleaned_signal, threshold=5, method='clip')\n",
    "                \n",
    "                # Calculate features on cleaned data\n",
    "                energy = np.sum(cleaned_signal ** 2)\n",
    "                rms = np.sqrt(np.mean(cleaned_signal ** 2))\n",
    "                peak = np.max(np.abs(cleaned_signal))\n",
    "                std = np.std(cleaned_signal)\n",
    "                \n",
    "                # Frequency domain features\n",
    "                sr = 1 / df['timestamp'].diff().mean()\n",
    "                freqs, power = compute_spectrum(cleaned_signal, sr)\n",
    "                dominant_freq = freqs[np.argmax(power)]\n",
    "                \n",
    "                vehicle_features.append({\n",
    "                    'vehicle': vehicle,\n",
    "                    'energy': energy,\n",
    "                    'rms': rms,\n",
    "                    'peak': peak,\n",
    "                    'std': std,\n",
    "                    'dominant_freq': dominant_freq\n",
    "                })\n",
    "                print(f\"Processed (cleaned): {vehicle}\")\n",
    "\n",
    "vehicle_df = pd.DataFrame(vehicle_features)\n",
    "print(\"\\nVehicle Type Features (from cleaned data):\")\n",
    "print(vehicle_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1880401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vehicle type characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RMS comparison\n",
    "axes[0, 0].bar(vehicle_df['vehicle'], vehicle_df['rms'], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('RMS Amplitude by Vehicle Type', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMS')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Peak amplitude comparison\n",
    "axes[0, 1].bar(vehicle_df['vehicle'], vehicle_df['peak'], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_title('Peak Amplitude by Vehicle Type', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Peak')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Energy comparison (log scale)\n",
    "axes[1, 0].bar(vehicle_df['vehicle'], vehicle_df['energy'], color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].set_title('Signal Energy by Vehicle Type (log scale)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Energy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Dominant frequency comparison\n",
    "axes[1, 1].bar(vehicle_df['vehicle'], vehicle_df['dominant_freq'], color='plum', edgecolor='black')\n",
    "axes[1, 1].set_title('Dominant Frequency by Vehicle Type', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Frequency (Hz)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade0747",
   "metadata": {},
   "source": [
    "## 8. LVC Toolkit Integration Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Multi-Modal Sensor Data**\n",
    "   - Audio (AUD): Stereo acoustic data at 16 kHz (some vehicles)\n",
    "   - Seismic (EHZ, ENE, ENN, ENZ): Ground vibration sensors at ~100 Hz\n",
    "   - Complementary information for robust detection\n",
    "\n",
    "2. **Vehicle Classification**\n",
    "   - Different vehicles produce distinct acoustic/seismic signatures\n",
    "   - Features: RMS, peak amplitude, energy, spectral content\n",
    "   - Machine learning potential for automatic classification\n",
    "\n",
    "3. **Dataset Composition**\n",
    "   - Multiple vehicle types: bicycle, motor, mustang, pickup, scooter, tesla, walk, and more\n",
    "   - Multiple recording sessions per vehicle type\n",
    "   - Consistent sensor array across recordings\n",
    "\n",
    "### LVC Toolkit Applications:\n",
    "\n",
    "#### 1. **Live Component**\n",
    "   - Real-time vehicle detection and tracking\n",
    "   - Ground truth data for validation\n",
    "   - Sensor fusion algorithms\n",
    "\n",
    "#### 2. **Virtual Component**\n",
    "   - Signal simulation based on real data\n",
    "   - Physics-based propagation models\n",
    "   - Synthetic data generation for training\n",
    "\n",
    "#### 3. **Constructive Component**\n",
    "   - Agent behavior modeling\n",
    "   - Detection probability models\n",
    "   - Mission planning and analysis\n",
    "\n",
    "### Recommended Features for ML Models:\n",
    "\n",
    "1. **Time Domain:**\n",
    "   - RMS amplitude\n",
    "   - Peak values\n",
    "   - Zero-crossing rate\n",
    "   - Signal energy\n",
    "   \n",
    "2. **Frequency Domain:**\n",
    "   - Dominant frequency\n",
    "   - Spectral centroid\n",
    "   - Spectral rolloff\n",
    "   - MFCCs (for audio)\n",
    "   \n",
    "3. **Multi-Sensor:**\n",
    "   - Cross-correlation between sensors\n",
    "   - Sensor fusion features\n",
    "   - Directional information (from E/N components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd78d49",
   "metadata": {},
   "source": [
    "## 9. Feature Extraction for ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abe0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(signal_data, sample_rate=100):\n",
    "    \"\"\"Extract comprehensive features from signal data\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Time domain features\n",
    "    features['mean'] = np.mean(signal_data)\n",
    "    features['std'] = np.std(signal_data)\n",
    "    features['rms'] = np.sqrt(np.mean(signal_data ** 2))\n",
    "    features['peak'] = np.max(np.abs(signal_data))\n",
    "    features['peak_to_peak'] = np.ptp(signal_data)\n",
    "    features['energy'] = np.sum(signal_data ** 2)\n",
    "    features['zero_crossing_rate'] = np.sum(np.diff(np.sign(signal_data)) != 0) / len(signal_data)\n",
    "    \n",
    "    # Statistical features\n",
    "    features['skewness'] = pd.Series(signal_data).skew()\n",
    "    features['kurtosis'] = pd.Series(signal_data).kurtosis()\n",
    "    features['q25'] = np.percentile(signal_data, 25)\n",
    "    features['q75'] = np.percentile(signal_data, 75)\n",
    "    \n",
    "    # Frequency domain features\n",
    "    freqs, power = compute_spectrum(signal_data, sample_rate)\n",
    "    features['dominant_freq'] = freqs[np.argmax(power)]\n",
    "    features['spectral_centroid'] = np.sum(freqs * power) / np.sum(power)\n",
    "    cumsum = np.cumsum(power)\n",
    "    features['spectral_rolloff'] = freqs[np.where(cumsum >= 0.85 * cumsum[-1])[0][0]]\n",
    "    features['spectral_bandwidth'] = np.sqrt(np.sum(((freqs - features['spectral_centroid']) ** 2) * power) / np.sum(power))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test feature extraction on cleaned data\n",
    "test_signal = seismic_cleaned_no_outliers[:10000]\n",
    "test_features = extract_features(test_signal)\n",
    "\n",
    "print(\"Extracted Features (from cleaned data):\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in test_features.items():\n",
    "    print(f\"{key:25s}: {value:12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee381d2",
   "metadata": {},
   "source": [
    "## 10. Next Steps for LVC Integration\n",
    "\n",
    "### Immediate Actions:\n",
    "1. Build feature extraction pipeline for all vehicle datasets\n",
    "2. Create labeled dataset for vehicle classification\n",
    "3. Train baseline ML models (Random Forest, SVM, Neural Networks)\n",
    "4. Develop real-time detection algorithms\n",
    "5. Integrate with M3NVC dataset for expanded training data\n",
    "\n",
    "### Integration Tasks:\n",
    "1. **Data Interface**: Create standardized data loaders for LVC Toolkit\n",
    "2. **Feature Service**: Deploy feature extraction as microservice\n",
    "3. **Classification Service**: Real-time vehicle classification API\n",
    "4. **Visualization Dashboard**: Live monitoring and analysis\n",
    "5. **Simulation Module**: Generate synthetic FOCAL-like data\n",
    "\n",
    "### Performance Metrics:\n",
    "- Classification accuracy by vehicle type\n",
    "- Processing latency for real-time operation\n",
    "- False alarm rate\n",
    "- Sensor fusion performance gain\n",
    "- Cross-dataset generalization (MOD_vehicle + M3NVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22765b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"MOD_vehicle Data Exploration Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset Structure:\")\n",
    "print(f\"  Vehicle Types: {len(vehicle_types)}\")\n",
    "print(f\"  Sensor Modalities: Audio + Seismic (EHZ, ENE, ENN, ENZ)\")\n",
    "print(f\"\\nData Characteristics:\")\n",
    "print(f\"  Seismic Sample Rate: ~100 Hz\")\n",
    "print(f\"  Audio Sample Rate: 16,000 Hz (where available)\")\n",
    "print(f\"  Seismic Channels: 4 (3D + vertical)\")\n",
    "print(f\"\\nVehicle Types Analyzed: {', '.join(vehicle_df['vehicle'].values)}\")\n",
    "print(f\"\\nServer Path: {BASE_PATH}\")\n",
    "print(f\"\\nReady for LVC Toolkit integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
