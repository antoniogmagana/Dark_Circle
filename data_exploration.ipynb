{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d84437",
   "metadata": {},
   "source": [
    "# IoBT Moving Object Detection - Data Exploration and Analysis\n",
    "\n",
    "## Seismic and Acoustic Data Summary Statistics\n",
    "\n",
    "This notebook performs initial data exploration and statistical analysis of seismic (EHZ) and acoustic (AUD16000) sensor data collected from various vehicle movement experiments.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- **Seismic Data (ehz.csv)**: Vertical component seismometer readings\n",
    "- **Acoustic Data (aud16000.csv)**: Audio recordings at 16 kHz sampling rate (stored via Git LFS)\n",
    "- **GPS Data (gps.csv)**: Vehicle position tracking\n",
    "- **Vehicle Types**: Polaris, Silverado, Warhog\n",
    "- **Conditions**: Line of sight and no line of sight scenarios\n",
    "\n",
    "**âš ï¸ Note for Git Users:** Large audio files are managed with Git LFS. After cloning, run `git lfs pull` to download all data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35a916",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from scipy import stats  # Used for skewness and kurtosis calculations\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552924e",
   "metadata": {},
   "source": [
    "## 2. Define Data Directory and Explore Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base data directory\n",
    "# GitHub Repository: https://github.com/antoniogmagana/Dark_Circle.git\n",
    "# \n",
    "# SETUP INSTRUCTIONS FOR USERS:\n",
    "# 1. Clone the repository: git clone https://github.com/antoniogmagana/Dark_Circle.git\n",
    "# 2. Place your data in: Dark_Circle/raw_data/\n",
    "# 3. Expected structure:\n",
    "#    Dark_Circle/\n",
    "#    â”œâ”€â”€ Data exploration.ipynb (this notebook)\n",
    "#    â””â”€â”€ raw_data/\n",
    "#        â”œâ”€â”€ Polaris0150pm/\n",
    "#        â”œâ”€â”€ Polaris0215pm/\n",
    "#        â”œâ”€â”€ Silverado0255pm/\n",
    "#        â””â”€â”€ ... (other experiment folders)\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "notebook_dir = Path.cwd()  # Default to current working directory\n",
    "\n",
    "try:\n",
    "    # Try to get notebook path from ipython (available in Jupyter environments)\n",
    "    ipython = get_ipython()  # type: ignore\n",
    "    if ipython is not None:\n",
    "        notebook_file = ipython.user_ns.get('__vsc_ipynb_file__', '')\n",
    "        if notebook_file:\n",
    "            notebook_path = Path(notebook_file)\n",
    "            if notebook_path.exists():\n",
    "                notebook_dir = notebook_path.parent\n",
    "except (NameError, AttributeError):\n",
    "    # Not in a Jupyter environment or get_ipython not available\n",
    "    pass\n",
    "\n",
    "# Expected structure: raw_data should be in same directory as notebook\n",
    "base_dir = notebook_dir / 'raw_data'\n",
    "\n",
    "# Allow override via environment variable\n",
    "if 'DATA_DIR' in os.environ:\n",
    "    base_dir = Path(os.environ['DATA_DIR'])\n",
    "\n",
    "print(f\"ðŸ“ Notebook directory: {notebook_dir}\")\n",
    "print(f\"ðŸ“ Data directory: {base_dir}\")\n",
    "print(f\"ðŸ“ Directory exists: {base_dir.exists()}\")\n",
    "\n",
    "if not base_dir.exists():\n",
    "    print(f\"\\nâš ï¸  Data directory not found!\")\n",
    "    print(f\"\\nðŸ’¡ Setup Instructions:\")\n",
    "    print(f\"   1. Clone repository: git clone https://github.com/antoniogmagana/Dark_Circle.git\")\n",
    "    print(f\"   2. Move this notebook to: Dark_Circle/\")\n",
    "    print(f\"   3. Create directory: Dark_Circle/raw_data/\")\n",
    "    print(f\"   4. Place your experiment folders in: Dark_Circle/raw_data/\")\n",
    "    print(f\"\\n   Or set DATA_DIR environment variable to point to your data location.\")\n",
    "print()\n",
    "\n",
    "# List all experiment folders (only if directory exists)\n",
    "experiment_folders = []\n",
    "if base_dir.exists():\n",
    "    experiment_folders = [f.name for f in base_dir.iterdir() if f.is_dir()]\n",
    "    experiment_folders.sort()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AVAILABLE EXPERIMENT FOLDERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if experiment_folders:\n",
    "    for i, folder in enumerate(experiment_folders, 1):\n",
    "        folder_path = base_dir / folder\n",
    "        rs1_path = folder_path / 'rs1'\n",
    "        \n",
    "        # Filter out system files like .DS_Store\n",
    "        files = []\n",
    "        if rs1_path.exists():\n",
    "            files = [f.name for f in rs1_path.iterdir() \n",
    "                    if f.is_file() and not f.name.startswith('.')]\n",
    "            files.sort()\n",
    "        \n",
    "        print(f\"{i}. {folder}\")\n",
    "        if files:\n",
    "            print(f\"   Files: {', '.join(files)}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  No data files found in rs1/ subdirectory\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Total experiments: {len(experiment_folders)}\")\n",
    "else:\n",
    "    print(\"âŒ No experiment folders found.\")\n",
    "    print(f\"Expected location: {base_dir}\")\n",
    "    print(\"\\nPlease ensure your data is organized as follows:\")\n",
    "    print(\"  Dark_Circle/\")\n",
    "    print(\"  â”œâ”€â”€ Data exploration.ipynb\")\n",
    "    print(\"  â””â”€â”€ raw_data/\")\n",
    "    print(\"      â”œâ”€â”€ Polaris0150pm/\")\n",
    "    print(\"      â”‚   â””â”€â”€ rs1/\")\n",
    "    print(\"      â”‚       â”œâ”€â”€ aud16000.csv\")\n",
    "    print(\"      â”‚       â”œâ”€â”€ ehz.csv\")\n",
    "    print(\"      â”‚       â””â”€â”€ gps.csv\")\n",
    "    print(\"      â””â”€â”€ ... (other experiment folders)\")\n",
    "    \n",
    "# Convert base_dir back to string for compatibility with rest of notebook\n",
    "base_dir = str(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a4e44",
   "metadata": {},
   "source": [
    "## 3. Load All Seismic Data (EHZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store seismic data\n",
    "seismic_data = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING SEISMIC DATA (EHZ)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for folder in experiment_folders:\n",
    "    ehz_path = os.path.join(base_dir, folder, 'rs1', 'ehz.csv')\n",
    "    \n",
    "    if os.path.exists(ehz_path):\n",
    "        try:\n",
    "            # Load with space delimiter, no header\n",
    "            df = pd.read_csv(ehz_path, sep=' ', header=None, names=['amplitude', 'timestamp'])\n",
    "            \n",
    "            # Validate data\n",
    "            if df.empty:\n",
    "                print(f\"âš ï¸  {folder}: File is empty\\n\")\n",
    "                continue\n",
    "            \n",
    "            seismic_data[folder] = df\n",
    "            print(f\"âœ“ {folder}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {folder}: Error loading file - {str(e)}\\n\")\n",
    "    else:\n",
    "        print(f\"âœ— {folder}: File not found at {ehz_path}\\n\")\n",
    "\n",
    "print(f\"Successfully loaded {len(seismic_data)} seismic datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the first seismic dataset\n",
    "if seismic_data:\n",
    "    first_key = list(seismic_data.keys())[0]\n",
    "    print(f\"Sample data from {first_key}:\\n\")\n",
    "    print(seismic_data[first_key].head(10))\n",
    "    print(f\"\\nData types:\\n{seismic_data[first_key].dtypes}\")\n",
    "    print(f\"\\nBasic info:\\n{seismic_data[first_key].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdd405",
   "metadata": {},
   "source": [
    "## 4. Load All Acoustic Data (AUD16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f997c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store acoustic data\n",
    "acoustic_data = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING ACOUSTIC DATA (AUD16000)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for folder in experiment_folders:\n",
    "    aud_path = os.path.join(base_dir, folder, 'rs1', 'aud16000.csv')\n",
    "    \n",
    "    if os.path.exists(aud_path):\n",
    "        try:\n",
    "            # Load without header and assign column name\n",
    "            df = pd.read_csv(aud_path, header=None, names=['amplitude'])\n",
    "            \n",
    "            # Validate data\n",
    "            if df.empty:\n",
    "                print(f\"âš ï¸  {folder}: File is empty\\n\")\n",
    "                continue\n",
    "            \n",
    "            acoustic_data[folder] = df\n",
    "            print(f\"âœ“ {folder}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {folder}: Error loading file - {str(e)}\\n\")\n",
    "    else:\n",
    "        print(f\"âœ— {folder}: File not found at {aud_path}\\n\")\n",
    "\n",
    "print(f\"Successfully loaded {len(acoustic_data)} acoustic datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50912a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of the first acoustic dataset\n",
    "if acoustic_data:\n",
    "    first_key = list(acoustic_data.keys())[0]\n",
    "    print(f\"Sample data from {first_key}:\\n\")\n",
    "    print(acoustic_data[first_key].head(10))\n",
    "    print(f\"\\nData types:\\n{acoustic_data[first_key].dtypes}\")\n",
    "    print(f\"\\nBasic info:\\n{acoustic_data[first_key].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8290981",
   "metadata": {},
   "source": [
    "## 5. Load GPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d43e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store GPS data\n",
    "gps_data = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING GPS DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for folder in experiment_folders:\n",
    "    gps_path = os.path.join(base_dir, folder, 'rs1', 'gps.csv')\n",
    "    \n",
    "    if os.path.exists(gps_path):\n",
    "        try:\n",
    "            # Load without header and assign column names\n",
    "            df = pd.read_csv(gps_path, header=None, names=['timestamp', 'latitude', 'longitude', 'altitude'])\n",
    "            \n",
    "            # Validate data\n",
    "            if df.empty:\n",
    "                print(f\"âš ï¸  {folder}: File is empty\\n\")\n",
    "                continue\n",
    "            \n",
    "            gps_data[folder] = df\n",
    "            print(f\"âœ“ {folder}\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— {folder}: Error loading file - {str(e)}\\n\")\n",
    "    else:\n",
    "        print(f\"âœ— {folder}: File not found at {gps_path}\\n\")\n",
    "\n",
    "print(f\"Successfully loaded {len(gps_data)} GPS datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a8dd8",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning and Quality Analysis\n",
    "\n",
    "Raw sensor data often contains artifacts that need to be cleaned before analysis. This section performs data cleaning and compares results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3175058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive statistics for seismic data\n",
    "seismic_stats = []\n",
    "\n",
    "for exp_name, df in seismic_data.items():\n",
    "    # Identify numeric columns (assuming signal data columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data_col = df[col].dropna()\n",
    "        \n",
    "        if len(data_col) > 0:\n",
    "            stats_dict = {\n",
    "                'Experiment': exp_name,\n",
    "                'Column': col,\n",
    "                'Count': len(data_col),\n",
    "                'Mean': data_col.mean(),\n",
    "                'Median': data_col.median(),\n",
    "                'Std Dev': data_col.std(),\n",
    "                'Min': data_col.min(),\n",
    "                'Max': data_col.max(),\n",
    "                'Range': data_col.max() - data_col.min(),\n",
    "                'Q25': data_col.quantile(0.25),\n",
    "                'Q75': data_col.quantile(0.75),\n",
    "                'IQR': data_col.quantile(0.75) - data_col.quantile(0.25),\n",
    "                'Skewness': stats.skew(data_col),\n",
    "                'Kurtosis': stats.kurtosis(data_col),\n",
    "                'RMS': np.sqrt(np.mean(data_col**2)),\n",
    "            }\n",
    "            seismic_stats.append(stats_dict)\n",
    "\n",
    "seismic_stats_df = pd.DataFrame(seismic_stats)\n",
    "print(\"=\" * 100)\n",
    "print(\"SEISMIC DATA (EHZ) - COMPREHENSIVE STATISTICS\")\n",
    "print(\"=\" * 100)\n",
    "print(seismic_stats_df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600104c5",
   "metadata": {},
   "source": [
    "## 6A. Data Cleaning - Seismic Data\n",
    "\n",
    "Remove DC offset and outliers from seismic data to reveal true signal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a153a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean seismic data by removing DC offset and outliers\n",
    "seismic_data_cleaned = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SEISMIC DATA CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp_name, df in seismic_data.items():\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove DC offset (subtract mean)\n",
    "    original_mean = df_clean['amplitude'].mean()\n",
    "    df_clean['amplitude_clean'] = df_clean['amplitude'] - original_mean\n",
    "    \n",
    "    # Remove outliers using IQR method\n",
    "    Q1 = df_clean['amplitude_clean'].quantile(0.25)\n",
    "    Q3 = df_clean['amplitude_clean'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers_count = ((df_clean['amplitude_clean'] < lower_bound) | \n",
    "                     (df_clean['amplitude_clean'] > upper_bound)).sum()\n",
    "    \n",
    "    # Replace outliers with median\n",
    "    median_val = df_clean['amplitude_clean'].median()\n",
    "    df_clean.loc[(df_clean['amplitude_clean'] < lower_bound) | \n",
    "                 (df_clean['amplitude_clean'] > upper_bound), \n",
    "                 'amplitude_clean'] = median_val\n",
    "    \n",
    "    seismic_data_cleaned[exp_name] = df_clean\n",
    "    \n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Original mean (DC offset): {original_mean:.2f}\")\n",
    "    print(f\"  Cleaned mean: {df_clean['amplitude_clean'].mean():.2e}\")\n",
    "    print(f\"  Cleaned std dev: {df_clean['amplitude_clean'].std():.2f}\")\n",
    "    print(f\"  Outliers removed: {outliers_count} ({outliers_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Seismic data cleaning complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc105f6f",
   "metadata": {},
   "source": [
    "## 6B. Data Cleaning - Acoustic Data\n",
    "\n",
    "Clean acoustic data by removing extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e95c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean acoustic data by removing outliers\n",
    "acoustic_data_cleaned = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ACOUSTIC DATA CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp_name, df in acoustic_data.items():\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Acoustic data is typically already AC-coupled, but remove extreme outliers\n",
    "    Q1 = df_clean['amplitude'].quantile(0.25)\n",
    "    Q3 = df_clean['amplitude'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers_count = ((df_clean['amplitude'] < lower_bound) | \n",
    "                     (df_clean['amplitude'] > upper_bound)).sum()\n",
    "    \n",
    "    # Replace outliers with median\n",
    "    median_val = df_clean['amplitude'].median()\n",
    "    df_clean['amplitude_clean'] = df_clean['amplitude'].copy()\n",
    "    df_clean.loc[(df_clean['amplitude'] < lower_bound) | \n",
    "                 (df_clean['amplitude'] > upper_bound), \n",
    "                 'amplitude_clean'] = median_val\n",
    "    \n",
    "    acoustic_data_cleaned[exp_name] = df_clean\n",
    "    \n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Original mean: {df['amplitude'].mean():.2f}\")\n",
    "    print(f\"  Cleaned mean: {df_clean['amplitude_clean'].mean():.2f}\")\n",
    "    print(f\"  Cleaned std dev: {df_clean['amplitude_clean'].std():.2f}\")\n",
    "    print(f\"  Outliers removed: {outliers_count} ({outliers_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Acoustic data cleaning complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3359c",
   "metadata": {},
   "source": [
    "## 6C. Data Cleaning Impact Comparison\n",
    "\n",
    "Compare statistics before and after cleaning to see the impact of removing DC offset and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194974a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Before and After Cleaning\n",
    "print(\"=\" * 100)\n",
    "print(\"DATA CLEANING IMPACT SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nðŸ“Š SEISMIC DATA:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"Before Cleaning (Raw RMS with DC offset):\")\n",
    "for exp_name, df in seismic_data.items():\n",
    "    raw_rms = np.sqrt(np.mean(df['amplitude'].dropna()**2))\n",
    "    print(f\"  {exp_name}: {raw_rms:.2f}\")\n",
    "\n",
    "print(\"\\nAfter Cleaning (DC-removed RMS):\")\n",
    "for exp_name, df in seismic_data_cleaned.items():\n",
    "    clean_rms = np.sqrt(np.mean(df['amplitude_clean'].dropna()**2))\n",
    "    print(f\"  {exp_name}: {clean_rms:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š KEY INSIGHT:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"âœ“ Raw seismic data had large DC offset (~16,280) masking true signal differences\")\n",
    "print(\"âœ“ After DC removal, actual vibration patterns are now visible and distinguishable\")\n",
    "print(\"âœ“ Cleaned data shows clear variation between different vehicles and conditions\")\n",
    "print(\"âœ“ Standard deviation now accurately represents signal energy, not measurement bias\")\n",
    "\n",
    "print(\"\\nðŸ“Š ACOUSTIC DATA:\")\n",
    "print(\"-\" * 100)\n",
    "print(\"âœ“ Acoustic sensors are naturally AC-coupled (no DC offset)\")\n",
    "print(\"âœ“ Cleaning focused on outlier removal for more accurate statistics\")\n",
    "print(\"âœ“ Vehicle signatures remain distinct and measurable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223da3c",
   "metadata": {},
   "source": [
    "## 6D. Cleaned Data Statistics Summary\n",
    "\n",
    "Create a summary dataframe with cleaned data statistics for vehicle and condition comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe from cleaned data for analysis\n",
    "cleaned_comparison = []\n",
    "\n",
    "for exp_name in experiment_folders:\n",
    "    if exp_name in seismic_data_cleaned and exp_name in acoustic_data_cleaned:\n",
    "        # Cleaned seismic stats\n",
    "        seismic_clean = seismic_data_cleaned[exp_name]['amplitude_clean'].dropna()\n",
    "        seismic_rms_clean = np.sqrt(np.mean(seismic_clean**2))\n",
    "        seismic_std = seismic_clean.std()\n",
    "        seismic_peak = np.abs(seismic_clean).max()\n",
    "        \n",
    "        # Cleaned acoustic stats\n",
    "        acoustic_clean = acoustic_data_cleaned[exp_name]['amplitude_clean'].dropna()\n",
    "        acoustic_rms_clean = np.sqrt(np.mean(acoustic_clean**2))\n",
    "        acoustic_std = acoustic_clean.std()\n",
    "        acoustic_peak = np.abs(acoustic_clean).max()\n",
    "        \n",
    "        # Determine vehicle and condition\n",
    "        vehicle = 'Unknown'\n",
    "        if 'Polaris' in exp_name:\n",
    "            vehicle = 'Polaris'\n",
    "        elif 'Silverado' in exp_name:\n",
    "            vehicle = 'Silverado'\n",
    "        elif 'Warhog' in exp_name:\n",
    "            vehicle = 'Warhog'\n",
    "        \n",
    "        los_condition = 'No LOS' if 'NoLineOfSight' in exp_name else 'LOS'\n",
    "        \n",
    "        cleaned_comparison.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Vehicle': vehicle,\n",
    "            'Condition': los_condition,\n",
    "            'Seismic RMS': seismic_rms_clean,\n",
    "            'Seismic Std Dev': seismic_std,\n",
    "            'Seismic Peak': seismic_peak,\n",
    "            'Acoustic RMS': acoustic_rms_clean,\n",
    "            'Acoustic Std Dev': acoustic_std,\n",
    "            'Acoustic Peak': acoustic_peak\n",
    "        })\n",
    "\n",
    "cleaned_df = pd.DataFrame(cleaned_comparison)\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"CLEANED DATA - SIGNAL VARIATION ANALYSIS\")\n",
    "print(\"=\" * 120)\n",
    "print(cleaned_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"CLEANED DATA - AVERAGE BY VEHICLE TYPE\")\n",
    "print(\"=\" * 120)\n",
    "vehicle_summary = cleaned_df.groupby('Vehicle')[['Seismic RMS', 'Seismic Std Dev', \n",
    "                                                   'Acoustic RMS', 'Acoustic Std Dev']].mean()\n",
    "print(vehicle_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"CLEANED DATA - AVERAGE BY CONDITION\")\n",
    "print(\"=\" * 120)\n",
    "condition_summary = cleaned_df.groupby('Condition')[['Seismic RMS', 'Seismic Std Dev', \n",
    "                                                       'Acoustic RMS', 'Acoustic Std Dev']].mean()\n",
    "print(condition_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6e8b5",
   "metadata": {},
   "source": [
    "## 6E. Before/After Cleaning: Visual Comparison\n",
    "\n",
    "Visual side-by-side comparison of raw and cleaned signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebe78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of raw vs cleaned data\n",
    "print(\"=\" * 100)\n",
    "print(\"BEFORE/AFTER CLEANING: VISUAL COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Select first 3 experiments for comparison\n",
    "experiments_to_compare = list(seismic_data.keys())[:3]\n",
    "\n",
    "# Create side-by-side comparison for seismic data\n",
    "fig, axes = plt.subplots(len(experiments_to_compare), 2, figsize=(18, 4*len(experiments_to_compare)))\n",
    "\n",
    "for idx, exp_name in enumerate(experiments_to_compare):\n",
    "    if exp_name in seismic_data and exp_name in seismic_data_cleaned:\n",
    "        # Raw data\n",
    "        raw_df = seismic_data[exp_name]\n",
    "        raw_signal = raw_df['amplitude'].values[:10000]  # First 10k samples\n",
    "        \n",
    "        axes[idx, 0].plot(raw_signal, linewidth=0.5, color='darkred', alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'RAW Seismic - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Amplitude (with DC offset)')\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        axes[idx, 0].axhline(y=raw_signal.mean(), color='blue', linestyle='--', \n",
    "                            linewidth=2, label=f'Mean: {raw_signal.mean():.1f}')\n",
    "        axes[idx, 0].legend()\n",
    "        \n",
    "        # Cleaned data\n",
    "        clean_df = seismic_data_cleaned[exp_name]\n",
    "        clean_signal = clean_df['amplitude_clean'].values[:10000]\n",
    "        \n",
    "        axes[idx, 1].plot(clean_signal, linewidth=0.5, color='darkblue', alpha=0.7)\n",
    "        axes[idx, 1].set_title(f'CLEANED Seismic - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 1].set_ylabel('Amplitude (DC removed)')\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "        axes[idx, 1].axhline(y=0, color='red', linestyle='--', \n",
    "                            linewidth=2, label='Zero reference')\n",
    "        axes[idx, 1].axhline(y=clean_signal.mean(), color='blue', linestyle='--', \n",
    "                            linewidth=1, label=f'Mean: {clean_signal.mean():.2f}')\n",
    "        axes[idx, 1].legend()\n",
    "\n",
    "axes[-1, 0].set_xlabel('Sample Index')\n",
    "axes[-1, 1].set_xlabel('Sample Index')\n",
    "\n",
    "plt.suptitle('Seismic Data: Before vs After DC Offset Removal', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison for acoustic data\n",
    "fig, axes = plt.subplots(len(experiments_to_compare), 2, figsize=(18, 4*len(experiments_to_compare)))\n",
    "\n",
    "for idx, exp_name in enumerate(experiments_to_compare):\n",
    "    if exp_name in acoustic_data and exp_name in acoustic_data_cleaned:\n",
    "        # Raw data\n",
    "        raw_df = acoustic_data[exp_name]\n",
    "        raw_signal = raw_df['amplitude'].values[:10000]  # First 10k samples\n",
    "        \n",
    "        axes[idx, 0].plot(raw_signal, linewidth=0.5, color='darkred', alpha=0.7)\n",
    "        axes[idx, 0].set_title(f'RAW Acoustic - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 0].set_ylabel('Amplitude')\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        raw_std = raw_signal.std()\n",
    "        axes[idx, 0].text(0.02, 0.98, f'Std Dev: {raw_std:.1f}', \n",
    "                         transform=axes[idx, 0].transAxes, \n",
    "                         verticalalignment='top', fontsize=10,\n",
    "                         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        # Cleaned data\n",
    "        clean_df = acoustic_data_cleaned[exp_name]\n",
    "        clean_signal = clean_df['amplitude_clean'].values[:10000]\n",
    "        \n",
    "        axes[idx, 1].plot(clean_signal, linewidth=0.5, color='darkgreen', alpha=0.7)\n",
    "        axes[idx, 1].set_title(f'CLEANED Acoustic - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx, 1].set_ylabel('Amplitude (outliers removed)')\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "        clean_std = clean_signal.std()\n",
    "        axes[idx, 1].text(0.02, 0.98, f'Std Dev: {clean_std:.1f}', \n",
    "                         transform=axes[idx, 1].transAxes, \n",
    "                         verticalalignment='top', fontsize=10,\n",
    "                         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "axes[-1, 0].set_xlabel('Sample Index')\n",
    "axes[-1, 1].set_xlabel('Sample Index')\n",
    "\n",
    "plt.suptitle('Acoustic Data: Before vs After Outlier Removal', \n",
    "            fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visual comparison complete\")\n",
    "print(\"  â†’ Seismic: DC offset (~16,280) removed, signal now centered at zero\")\n",
    "print(\"  â†’ Acoustic: Outliers removed, cleaner signal with preserved dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b5a495",
   "metadata": {},
   "source": [
    "## 7. Acoustic Data Statistics (After Cleaning)\n",
    "\n",
    "Comprehensive statistical analysis of cleaned acoustic sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2dc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive statistics for CLEANED acoustic data\n",
    "acoustic_stats = []\n",
    "\n",
    "for exp_name, df in acoustic_data_cleaned.items():\n",
    "    # Identify numeric columns (assuming signal data columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data_col = df[col].dropna()\n",
    "        \n",
    "        if len(data_col) > 0:\n",
    "            stats_dict = {\n",
    "                'Experiment': exp_name,\n",
    "                'Column': col,\n",
    "                'Count': len(data_col),\n",
    "                'Mean': data_col.mean(),\n",
    "                'Median': data_col.median(),\n",
    "                'Std Dev': data_col.std(),\n",
    "                'Min': data_col.min(),\n",
    "                'Max': data_col.max(),\n",
    "                'Range': data_col.max() - data_col.min(),\n",
    "                'Q25': data_col.quantile(0.25),\n",
    "                'Q75': data_col.quantile(0.75),\n",
    "                'IQR': data_col.quantile(0.75) - data_col.quantile(0.25),\n",
    "                'Skewness': stats.skew(data_col),\n",
    "                'Kurtosis': stats.kurtosis(data_col),\n",
    "                'RMS': np.sqrt(np.mean(data_col**2)),\n",
    "            }\n",
    "            acoustic_stats.append(stats_dict)\n",
    "\n",
    "acoustic_stats_df = pd.DataFrame(acoustic_stats)\n",
    "print(\"=\" * 100)\n",
    "print(\"ACOUSTIC DATA (AUD16000) - COMPREHENSIVE STATISTICS (CLEANED)\")\n",
    "print(\"=\" * 100)\n",
    "print(acoustic_stats_df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833a808",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment (Cleaned Data)\n",
    "\n",
    "Validate data quality after cleaning: check for missing values, duplicates, and data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality for CLEANED seismic data\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA QUALITY ASSESSMENT - SEISMIC (EHZ) - CLEANED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp_name, df in seismic_data_cleaned.items():\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Total rows: {len(df)}\")\n",
    "    print(f\"  Missing values per column:\")\n",
    "    missing = df.isnull().sum()\n",
    "    for col, count in missing.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {col}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    if missing.sum() == 0:\n",
    "        print(f\"    No missing values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"  Duplicate rows: {duplicates}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA QUALITY ASSESSMENT - ACOUSTIC (AUD16000) - CLEANED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp_name, df in acoustic_data_cleaned.items():\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    print(f\"  Total rows: {len(df)}\")\n",
    "    print(f\"  Missing values per column:\")\n",
    "    missing = df.isnull().sum()\n",
    "    for col, count in missing.items():\n",
    "        if count > 0:\n",
    "            print(f\"    {col}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "    if missing.sum() == 0:\n",
    "        print(f\"    No missing values\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"  Duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aeabd6",
   "metadata": {},
   "source": [
    "## 9. Time Series Visualization - Cleaned Seismic Data\n",
    "\n",
    "Visualize cleaned seismic waveforms to identify patterns and anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for all CLEANED seismic datasets\n",
    "fig, axes = plt.subplots(len(seismic_data_cleaned), 1, figsize=(15, 4*len(seismic_data_cleaned)))\n",
    "\n",
    "if len(seismic_data_cleaned) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (exp_name, df) in enumerate(seismic_data_cleaned.items()):\n",
    "    # Get the first numeric column (assuming it's the signal)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        \n",
    "        # Plot the signal\n",
    "        axes[idx].plot(df[signal_col], linewidth=0.5, color='darkblue', alpha=0.7)\n",
    "        axes[idx].set_title(f'Seismic Signal (Cleaned) - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Sample Index')\n",
    "        axes[idx].set_ylabel('Amplitude')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics as text\n",
    "        mean_val = df[signal_col].mean()\n",
    "        std_val = df[signal_col].std()\n",
    "        axes[idx].axhline(y=mean_val, color='r', linestyle='--', linewidth=1, label=f'Mean: {mean_val:.2e}')\n",
    "        axes[idx].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7393c7",
   "metadata": {},
   "source": [
    "## 10. Time Series Visualization - Acoustic Data (Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series for all CLEANED acoustic datasets\n",
    "fig, axes = plt.subplots(len(acoustic_data_cleaned), 1, figsize=(15, 4*len(acoustic_data_cleaned)))\n",
    "\n",
    "if len(acoustic_data_cleaned) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (exp_name, df) in enumerate(acoustic_data_cleaned.items()):\n",
    "    # Get the first numeric column (assuming it's the signal)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        \n",
    "        # Plot the signal\n",
    "        axes[idx].plot(df[signal_col], linewidth=0.5, color='darkgreen', alpha=0.7)\n",
    "        axes[idx].set_title(f'Acoustic Signal (Cleaned) - {exp_name}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Sample Index')\n",
    "        axes[idx].set_ylabel('Amplitude')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics as text\n",
    "        mean_val = df[signal_col].mean()\n",
    "        std_val = df[signal_col].std()\n",
    "        axes[idx].axhline(y=mean_val, color='r', linestyle='--', linewidth=1, label=f'Mean: {mean_val:.2e}')\n",
    "        axes[idx].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a4c1d",
   "metadata": {},
   "source": [
    "## 11. Distribution Analysis - Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots comparing seismic data across experiments\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Prepare data for CLEANED seismic box plot\n",
    "seismic_box_data = []\n",
    "seismic_labels = []\n",
    "\n",
    "for exp_name, df in seismic_data_cleaned.items():  # type: ignore - defined in Section 6A\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        seismic_box_data.append(df[signal_col].dropna())\n",
    "        # Shorten label for better display\n",
    "        short_label = exp_name.replace('Polaris', 'Pol').replace('Silverado', 'Sil').replace('Warhog', 'War')\n",
    "        seismic_labels.append(short_label)\n",
    "\n",
    "# Prepare data for CLEANED acoustic box plot\n",
    "acoustic_box_data = []\n",
    "acoustic_labels = []\n",
    "\n",
    "for exp_name, df in acoustic_data_cleaned.items():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        acoustic_box_data.append(df[signal_col].dropna())\n",
    "        short_label = exp_name.replace('Polaris', 'Pol').replace('Silverado', 'Sil').replace('Warhog', 'War')\n",
    "        acoustic_labels.append(short_label)\n",
    "\n",
    "# Plot seismic\n",
    "bp1 = ax1.boxplot(seismic_box_data, labels=seismic_labels, patch_artist=True)\n",
    "for patch in bp1['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "ax1.set_title('Seismic Data Distribution Comparison (Cleaned)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_xlabel('Experiment')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot acoustic\n",
    "bp2 = ax2.boxplot(acoustic_box_data, labels=acoustic_labels, patch_artist=True)\n",
    "for patch in bp2['boxes']:\n",
    "    patch.set_facecolor('lightgreen')\n",
    "ax2.set_title('Acoustic Data Distribution Comparison (Cleaned)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.set_xlabel('Experiment')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebcb28",
   "metadata": {},
   "source": [
    "## 12. Distribution Analysis - Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95be76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for CLEANED seismic data\n",
    "n_experiments = len(seismic_data_cleaned)\n",
    "fig, axes = plt.subplots(2, (n_experiments + 1) // 2, figsize=(16, 8))\n",
    "axes = axes.flatten() if n_experiments > 1 else [axes]\n",
    "\n",
    "for idx, (exp_name, df) in enumerate(seismic_data_cleaned.items()):  # type: ignore - defined in Section 6A\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        data = df[signal_col].dropna()\n",
    "        \n",
    "        axes[idx].hist(data, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[idx].set_title(f'{exp_name}', fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Amplitude')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        axes[idx].legend()\n",
    "\n",
    "# Hide extra subplots if any\n",
    "for idx in range(len(seismic_data_cleaned), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Seismic Data Amplitude Distributions (Cleaned)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for CLEANED acoustic data\n",
    "n_experiments = len(acoustic_data_cleaned)\n",
    "fig, axes = plt.subplots(2, (n_experiments + 1) // 2, figsize=(16, 8))\n",
    "axes = axes.flatten() if n_experiments > 1 else [axes]\n",
    "\n",
    "for idx, (exp_name, df) in enumerate(acoustic_data_cleaned.items()):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        data = df[signal_col].dropna()\n",
    "        \n",
    "        axes[idx].hist(data, bins=50, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "        axes[idx].set_title(f'{exp_name}', fontsize=10, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Amplitude')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add vertical line for mean\n",
    "        axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "        axes[idx].legend()\n",
    "\n",
    "# Hide extra subplots if any\n",
    "for idx in range(len(acoustic_data_cleaned), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Acoustic Data Amplitude Distributions (Cleaned)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17749548",
   "metadata": {},
   "source": [
    "## 13. Signal Amplitude Analysis (Cleaned Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e08b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RMS values and peak amplitudes using CLEANED data\n",
    "amplitude_comparison = []\n",
    "\n",
    "# CLEANED Seismic amplitude analysis\n",
    "for exp_name, df in seismic_data_cleaned.items():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        data = df[signal_col].dropna()\n",
    "        \n",
    "        rms = np.sqrt(np.mean(data**2))\n",
    "        peak_amplitude = np.abs(data).max()\n",
    "        mean_abs = np.abs(data).mean()\n",
    "        \n",
    "        amplitude_comparison.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Type': 'Seismic',\n",
    "            'RMS': rms,\n",
    "            'Peak Amplitude': peak_amplitude,\n",
    "            'Mean Absolute': mean_abs,\n",
    "            'Peak-to-RMS Ratio': peak_amplitude / rms if rms != 0 else 0\n",
    "        })\n",
    "\n",
    "# CLEANED Acoustic amplitude analysis\n",
    "for exp_name, df in acoustic_data_cleaned.items():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        signal_col = numeric_cols[0]\n",
    "        data = df[signal_col].dropna()\n",
    "        \n",
    "        rms = np.sqrt(np.mean(data**2))\n",
    "        peak_amplitude = np.abs(data).max()\n",
    "        mean_abs = np.abs(data).mean()\n",
    "        \n",
    "        amplitude_comparison.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Type': 'Acoustic',\n",
    "            'RMS': rms,\n",
    "            'Peak Amplitude': peak_amplitude,\n",
    "            'Mean Absolute': mean_abs,\n",
    "            'Peak-to-RMS Ratio': peak_amplitude / rms if rms != 0 else 0\n",
    "        })\n",
    "\n",
    "amplitude_df = pd.DataFrame(amplitude_comparison)\n",
    "print(\"=\" * 100)\n",
    "print(\"SIGNAL AMPLITUDE ANALYSIS (CLEANED DATA)\")\n",
    "print(\"=\" * 100)\n",
    "print(amplitude_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec979a1",
   "metadata": {},
   "source": [
    "## 13.5. GPS Speed Analysis and Signal Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS Speed Analysis and Correlation with Signal Strength\n",
    "print(\"=\" * 100)\n",
    "print(\"GPS SPEED ANALYSIS AND SIGNAL CORRELATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze GPS data to calculate vehicle speeds\n",
    "gps_speed_analysis = []\n",
    "\n",
    "for exp_name, gps_df in gps_data.items():\n",
    "    if len(gps_df) > 1:\n",
    "        # Calculate time differences (assuming time column exists or using index)\n",
    "        if 'time' in gps_df.columns:\n",
    "            gps_df['time_diff'] = gps_df['time'].diff()\n",
    "        else:\n",
    "            # Assume samples at regular intervals\n",
    "            gps_df['time_diff'] = 1.0\n",
    "        \n",
    "        # Calculate distance between consecutive GPS points (simplified Euclidean distance)\n",
    "        lat_col = [col for col in gps_df.columns if 'lat' in col.lower()][0]\n",
    "        lon_col = [col for col in gps_df.columns if 'lon' in col.lower()][0]\n",
    "        \n",
    "        # Convert lat/lon to approximate distance in meters\n",
    "        # Using simplified formula: 1 degree lat â‰ˆ 111,111 meters, 1 degree lon â‰ˆ 111,111 * cos(lat) meters\n",
    "        gps_df['lat_diff'] = gps_df[lat_col].diff() * 111111\n",
    "        gps_df['lon_diff'] = gps_df[lon_col].diff() * 111111 * np.cos(np.radians(gps_df[lat_col]))\n",
    "        gps_df['distance'] = np.sqrt(gps_df['lat_diff']**2 + gps_df['lon_diff']**2)\n",
    "        \n",
    "        # Calculate speed (m/s)\n",
    "        gps_df['speed_ms'] = gps_df['distance'] / gps_df['time_diff']\n",
    "        \n",
    "        # Remove infinite and NaN values\n",
    "        valid_speeds = gps_df['speed_ms'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(valid_speeds) > 0:\n",
    "            avg_speed_ms = valid_speeds.mean()\n",
    "            max_speed_ms = valid_speeds.max()\n",
    "            avg_speed_kmh = avg_speed_ms * 3.6\n",
    "            max_speed_kmh = max_speed_ms * 3.6\n",
    "            \n",
    "            # Get corresponding signal strengths from cleaned data\n",
    "            seismic_rms = None\n",
    "            acoustic_rms = None\n",
    "            \n",
    "            if exp_name in seismic_data_cleaned:\n",
    "                seismic_signal = seismic_data_cleaned[exp_name]['amplitude_clean'].dropna()\n",
    "                seismic_rms = np.sqrt(np.mean(seismic_signal**2))\n",
    "            \n",
    "            if exp_name in acoustic_data_cleaned:\n",
    "                acoustic_signal = acoustic_data_cleaned[exp_name]['amplitude_clean'].dropna()\n",
    "                acoustic_rms = np.sqrt(np.mean(acoustic_signal**2))\n",
    "            \n",
    "            gps_speed_analysis.append({\n",
    "                'Experiment': exp_name,\n",
    "                'Avg Speed (km/h)': avg_speed_kmh,\n",
    "                'Max Speed (km/h)': max_speed_kmh,\n",
    "                'Seismic RMS': seismic_rms,\n",
    "                'Acoustic RMS': acoustic_rms,\n",
    "                'GPS Points': len(gps_df)\n",
    "            })\n",
    "\n",
    "if len(gps_speed_analysis) > 0:\n",
    "    gps_speed_df = pd.DataFrame(gps_speed_analysis)\n",
    "    print(\"\\nGPS Speed and Signal Correlation:\")\n",
    "    print(gps_speed_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate correlations\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"SPEED vs SIGNAL STRENGTH CORRELATIONS\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Filter rows with valid data\n",
    "    valid_data = gps_speed_df.dropna()\n",
    "    \n",
    "    if len(valid_data) > 2:\n",
    "        speed_seismic_corr = valid_data['Avg Speed (km/h)'].corr(valid_data['Seismic RMS'])\n",
    "        speed_acoustic_corr = valid_data['Avg Speed (km/h)'].corr(valid_data['Acoustic RMS'])\n",
    "        \n",
    "        print(f\"\\nSpeed vs Seismic RMS Correlation: {speed_seismic_corr:.4f}\")\n",
    "        print(f\"Speed vs Acoustic RMS Correlation: {speed_acoustic_corr:.4f}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Speed vs Seismic\n",
    "        axes[0].scatter(valid_data['Avg Speed (km/h)'], valid_data['Seismic RMS'], \n",
    "                       s=100, alpha=0.7, c='steelblue', edgecolors='black')\n",
    "        axes[0].set_xlabel('Average Speed (km/h)', fontsize=12)\n",
    "        axes[0].set_ylabel('Seismic RMS', fontsize=12)\n",
    "        axes[0].set_title(f'Speed vs Seismic Signal Strength (r={speed_seismic_corr:.3f})', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(valid_data) > 1:\n",
    "            z = np.polyfit(valid_data['Avg Speed (km/h)'], valid_data['Seismic RMS'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[0].plot(valid_data['Avg Speed (km/h)'], p(valid_data['Avg Speed (km/h)']), \n",
    "                        \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
    "            axes[0].legend()\n",
    "        \n",
    "        # Speed vs Acoustic\n",
    "        axes[1].scatter(valid_data['Avg Speed (km/h)'], valid_data['Acoustic RMS'], \n",
    "                       s=100, alpha=0.7, c='seagreen', edgecolors='black')\n",
    "        axes[1].set_xlabel('Average Speed (km/h)', fontsize=12)\n",
    "        axes[1].set_ylabel('Acoustic RMS', fontsize=12)\n",
    "        axes[1].set_title(f'Speed vs Acoustic Signal Strength (r={speed_acoustic_corr:.3f})', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(valid_data) > 1:\n",
    "            z = np.polyfit(valid_data['Avg Speed (km/h)'], valid_data['Acoustic RMS'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1].plot(valid_data['Avg Speed (km/h)'], p(valid_data['Avg Speed (km/h)']), \n",
    "                        \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
    "            axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nâœ“ Speed analysis complete\")\n",
    "        if abs(speed_seismic_corr) > 0.5:\n",
    "            print(f\"  Strong correlation between speed and seismic signals detected!\")\n",
    "        if abs(speed_acoustic_corr) > 0.5:\n",
    "            print(f\"  Strong correlation between speed and acoustic signals detected!\")\n",
    "    else:\n",
    "        print(\"\\nInsufficient data points for correlation analysis (need > 2 experiments)\")\n",
    "else:\n",
    "    print(\"\\nNo GPS speed data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vehicle type and condition from experiment names using CLEANED data\n",
    "vehicle_comparison = []\n",
    "\n",
    "for exp_name in experiment_folders:\n",
    "    # Determine vehicle type\n",
    "    if 'Polaris' in exp_name:\n",
    "        vehicle = 'Polaris'\n",
    "    elif 'Silverado' in exp_name:\n",
    "        vehicle = 'Silverado'\n",
    "    elif 'Warhog' in exp_name:\n",
    "        vehicle = 'Warhog'\n",
    "    else:\n",
    "        vehicle = 'Unknown'\n",
    "    \n",
    "    # Determine line of sight condition\n",
    "    los_condition = 'No LOS' if 'NoLineOfSight' in exp_name else 'LOS'\n",
    "    \n",
    "    # Get RMS values for this experiment using CLEANED data\n",
    "    seismic_rms = None\n",
    "    acoustic_rms = None\n",
    "    \n",
    "    if exp_name in seismic_data_cleaned:  # type: ignore - defined in Section 6A\n",
    "        df = seismic_data_cleaned[exp_name]\n",
    "        if 'amplitude_clean' in df.columns:\n",
    "            seismic_rms = np.sqrt(np.mean(df['amplitude_clean'].dropna()**2))\n",
    "    \n",
    "    if exp_name in acoustic_data_cleaned:  # type: ignore - defined in Section 6A\n",
    "        df = acoustic_data_cleaned[exp_name]\n",
    "        if 'amplitude_clean' in df.columns:\n",
    "            acoustic_rms = np.sqrt(np.mean(df['amplitude_clean'].dropna()**2))\n",
    "    \n",
    "    vehicle_comparison.append({\n",
    "        'Experiment': exp_name,\n",
    "        'Vehicle': vehicle,\n",
    "        'Condition': los_condition,\n",
    "        'Seismic RMS': seismic_rms,\n",
    "        'Acoustic RMS': acoustic_rms\n",
    "    })\n",
    "\n",
    "vehicle_df = pd.DataFrame(vehicle_comparison)\n",
    "print(\"=\" * 90)\n",
    "print(\"VEHICLE TYPE AND LINE-OF-SIGHT COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print(vehicle_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Group by vehicle type\n",
    "print(\"=\" * 90)\n",
    "print(\"AVERAGE RMS BY VEHICLE TYPE\")\n",
    "print(\"=\" * 90)\n",
    "grouped = vehicle_df.groupby('Vehicle')[['Seismic RMS', 'Acoustic RMS']].mean()\n",
    "print(grouped)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Group by condition\n",
    "print(\"=\" * 90)\n",
    "print(\"AVERAGE RMS BY LINE-OF-SIGHT CONDITION\")\n",
    "print(\"=\" * 90)\n",
    "grouped_cond = vehicle_df.groupby('Condition')[['Seismic RMS', 'Acoustic RMS']].mean()\n",
    "print(grouped_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d6c9e",
   "metadata": {},
   "source": [
    "## 15. Vehicle Comparison Visualization (Cleaned Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf76f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts for vehicle comparison using CLEANED data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Seismic RMS by vehicle\n",
    "vehicle_seismic_rms = cleaned_df.groupby('Vehicle')['Seismic RMS'].mean().sort_values()\n",
    "vehicle_seismic_std = cleaned_df.groupby('Vehicle')['Seismic Std Dev'].mean().loc[vehicle_seismic_rms.index]\n",
    "bars = axes[0].bar(vehicle_seismic_rms.index, vehicle_seismic_rms.values, color=['steelblue', 'coral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Average Seismic RMS by Vehicle Type (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('RMS Amplitude')\n",
    "axes[0].set_xlabel('Vehicle Type')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, vehicle_seismic_rms.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, f'{val:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Acoustic RMS by vehicle\n",
    "vehicle_acoustic_rms = cleaned_df.groupby('Vehicle')['Acoustic RMS'].mean().sort_values()\n",
    "vehicle_acoustic_std = cleaned_df.groupby('Vehicle')['Acoustic Std Dev'].mean().loc[vehicle_acoustic_rms.index]\n",
    "bars = axes[1].bar(vehicle_acoustic_rms.index, vehicle_acoustic_rms.values, color=['steelblue', 'coral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Acoustic RMS by Vehicle Type (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('RMS Amplitude')\n",
    "axes[1].set_xlabel('Vehicle Type')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, vehicle_acoustic_rms.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, f'{val:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create bar charts for line-of-sight comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Seismic RMS by condition\n",
    "cond_seismic_rms = cleaned_df.groupby('Condition')['Seismic RMS'].mean()\n",
    "bars = axes[0].bar(cond_seismic_rms.index, cond_seismic_rms.values, color=['skyblue', 'salmon'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Average Seismic RMS by Line-of-Sight Condition (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('RMS Amplitude')\n",
    "axes[0].set_xlabel('Condition')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, cond_seismic_rms.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, f'{val:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Acoustic RMS by condition\n",
    "cond_acoustic_rms = cleaned_df.groupby('Condition')['Acoustic RMS'].mean()\n",
    "bars = axes[1].bar(cond_acoustic_rms.index, cond_acoustic_rms.values, color=['skyblue', 'salmon'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Average Acoustic RMS by Line-of-Sight Condition (Cleaned)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('RMS Amplitude')\n",
    "axes[1].set_xlabel('Condition')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, cond_acoustic_rms.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, f'{val:.1f}', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadf6e2",
   "metadata": {},
   "source": [
    "## 16. GPS Trajectory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GPS trajectories\n",
    "if len(gps_data) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(gps_data)))\n",
    "    \n",
    "    for idx, (exp_name, df) in enumerate(gps_data.items()):\n",
    "        # Use the standardized column names\n",
    "        lat_col = 'latitude'\n",
    "        lon_col = 'longitude'\n",
    "        \n",
    "        if lat_col in df.columns and lon_col in df.columns:\n",
    "            # Plot trajectory\n",
    "            ax.plot(df[lon_col], df[lat_col], marker='o', markersize=3, \n",
    "                   label=exp_name, linewidth=2, color=colors[idx], alpha=0.7)\n",
    "            \n",
    "            # Mark start and end points\n",
    "            ax.scatter(df[lon_col].iloc[0], df[lat_col].iloc[0], \n",
    "                      s=100, marker='s', color=colors[idx], edgecolor='black', linewidth=2, zorder=5)\n",
    "            ax.scatter(df[lon_col].iloc[-1], df[lat_col].iloc[-1], \n",
    "                      s=100, marker='^', color=colors[idx], edgecolor='black', linewidth=2, zorder=5)\n",
    "    \n",
    "    ax.set_title('Vehicle GPS Trajectories', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude', fontsize=12)\n",
    "    ax.set_ylabel('Latitude', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display GPS summary statistics\n",
    "    print(\"=\" * 90)\n",
    "    print(\"GPS DATA SUMMARY\")\n",
    "    print(\"=\" * 90)\n",
    "    for exp_name, df in gps_data.items():\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        print(f\"  Number of GPS points: {len(df)}\")\n",
    "        print(f\"  Latitude range: {df['latitude'].min():.6f} to {df['latitude'].max():.6f}\")\n",
    "        print(f\"  Longitude range: {df['longitude'].min():.6f} to {df['longitude'].max():.6f}\")\n",
    "else:\n",
    "    print(\"No GPS data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61ad5b",
   "metadata": {},
   "source": [
    "## 17. Correlation Analysis: Acoustic vs Seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between acoustic and seismic RMS values using CLEANED data\n",
    "correlation_data = []\n",
    "\n",
    "for exp_name in experiment_folders:\n",
    "    if exp_name in seismic_data_cleaned and exp_name in acoustic_data_cleaned:  # type: ignore\n",
    "        # Get seismic RMS from CLEANED data\n",
    "        seismic_df = seismic_data_cleaned[exp_name]\n",
    "        if 'amplitude_clean' in seismic_df.columns:\n",
    "            seismic_signal = seismic_df['amplitude_clean'].dropna()\n",
    "            seismic_rms = np.sqrt(np.mean(seismic_signal**2))\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Get acoustic RMS from CLEANED data\n",
    "        acoustic_df = acoustic_data_cleaned[exp_name]\n",
    "        if 'amplitude_clean' in acoustic_df.columns:\n",
    "            acoustic_signal = acoustic_df['amplitude_clean'].dropna()\n",
    "            acoustic_rms = np.sqrt(np.mean(acoustic_signal**2))\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        correlation_data.append({\n",
    "            'Experiment': exp_name,\n",
    "            'Seismic_RMS': seismic_rms,\n",
    "            'Acoustic_RMS': acoustic_rms\n",
    "        })\n",
    "\n",
    "if len(correlation_data) > 0:\n",
    "    corr_df = pd.DataFrame(correlation_data)\n",
    "    \n",
    "    # Calculate Pearson correlation\n",
    "    correlation = corr_df['Seismic_RMS'].corr(corr_df['Acoustic_RMS'])\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"CORRELATION ANALYSIS: SEISMIC vs ACOUSTIC RMS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nPearson Correlation Coefficient: {correlation:.4f}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    if abs(correlation) > 0.7:\n",
    "        print(\"  Strong correlation\")\n",
    "    elif abs(correlation) > 0.4:\n",
    "        print(\"  Moderate correlation\")\n",
    "    else:\n",
    "        print(\"  Weak correlation\")\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    ax.scatter(corr_df['Seismic_RMS'], corr_df['Acoustic_RMS'], \n",
    "              s=100, alpha=0.6, color='purple', edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for idx, row in corr_df.iterrows():\n",
    "        short_name = row['Experiment'].replace('Polaris', 'P').replace('Silverado', 'S').replace('Warhog', 'W')\n",
    "        ax.annotate(short_name, (row['Seismic_RMS'], row['Acoustic_RMS']), \n",
    "                   fontsize=8, ha='right', va='bottom')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(corr_df['Seismic_RMS'], corr_df['Acoustic_RMS'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(corr_df['Seismic_RMS'], p(corr_df['Seismic_RMS']), \n",
    "           \"r--\", linewidth=2, label=f'Trend line (r={correlation:.3f})')\n",
    "    \n",
    "    ax.set_title('Correlation: Seismic RMS vs Acoustic RMS', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Seismic RMS', fontsize=12)\n",
    "    ax.set_ylabel('Acoustic RMS', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120f0e2",
   "metadata": {},
   "source": [
    "## 18. Frequency Domain Analysis (FFT) - Using Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform FFT analysis on a sample dataset using CLEANED data\n",
    "# Select the first experiment for detailed analysis\n",
    "if len(seismic_data_cleaned) > 0:\n",
    "    sample_exp = list(seismic_data_cleaned.keys())[0]\n",
    "    \n",
    "    # Seismic FFT (CLEANED)\n",
    "    seismic_df = seismic_data_cleaned[sample_exp]\n",
    "    seismic_signal = seismic_df['amplitude_clean'].dropna().values\n",
    "    \n",
    "    # Compute FFT\n",
    "    fft_seismic = np.fft.fft(seismic_signal)\n",
    "    freq_seismic = np.fft.fftfreq(len(seismic_signal))\n",
    "    \n",
    "    # Get positive frequencies only\n",
    "    pos_mask_seismic = freq_seismic > 0\n",
    "    freq_seismic_pos = freq_seismic[pos_mask_seismic]\n",
    "    fft_seismic_pos = np.abs(fft_seismic[pos_mask_seismic])\n",
    "    \n",
    "    # Acoustic FFT (CLEANED)\n",
    "    if sample_exp in acoustic_data_cleaned:\n",
    "        acoustic_df = acoustic_data_cleaned[sample_exp]\n",
    "        acoustic_signal = acoustic_df['amplitude_clean'].dropna().values\n",
    "        \n",
    "        # Compute FFT\n",
    "        fft_acoustic = np.fft.fft(acoustic_signal)\n",
    "        freq_acoustic = np.fft.fftfreq(len(acoustic_signal))\n",
    "        \n",
    "        # Get positive frequencies only\n",
    "        pos_mask_acoustic = freq_acoustic > 0\n",
    "        freq_acoustic_pos = freq_acoustic[pos_mask_acoustic]\n",
    "        fft_acoustic_pos = np.abs(fft_acoustic[pos_mask_acoustic])\n",
    "        \n",
    "        # Plot FFT results\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # Seismic FFT\n",
    "        axes[0].plot(freq_seismic_pos[:len(freq_seismic_pos)//10], \n",
    "                   fft_seismic_pos[:len(fft_seismic_pos)//10], \n",
    "                   color='darkblue', linewidth=1)\n",
    "        axes[0].set_title(f'Seismic Signal - Frequency Spectrum (Cleaned) - {sample_exp}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Frequency (Hz)')\n",
    "        axes[0].set_ylabel('Magnitude')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_xlim(left=0)\n",
    "        \n",
    "        # Acoustic FFT\n",
    "        axes[1].plot(freq_acoustic_pos[:len(freq_acoustic_pos)//10], \n",
    "                   fft_acoustic_pos[:len(fft_acoustic_pos)//10], \n",
    "                   color='darkgreen', linewidth=1)\n",
    "        axes[1].set_title(f'Acoustic Signal - Frequency Spectrum (Cleaned) - {sample_exp}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Frequency (Hz)')\n",
    "        axes[1].set_ylabel('Magnitude')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_xlim(left=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find dominant frequencies\n",
    "        top_n = 5\n",
    "        seismic_top_idx = np.argsort(fft_seismic_pos)[-top_n:][::-1]\n",
    "        acoustic_top_idx = np.argsort(fft_acoustic_pos)[-top_n:][::-1]\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(f\"DOMINANT FREQUENCIES - {sample_exp}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nTop {top_n} Seismic Frequencies:\")\n",
    "        for i, idx in enumerate(seismic_top_idx, 1):\n",
    "            print(f\"  {i}. {freq_seismic_pos[idx]:.4f} Hz (Magnitude: {fft_seismic_pos[idx]:.2e})\")\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Acoustic Frequencies:\")\n",
    "        for i, idx in enumerate(acoustic_top_idx, 1):\n",
    "            print(f\"  {i}. {freq_acoustic_pos[idx]:.4f} Hz (Magnitude: {fft_acoustic_pos[idx]:.2e})\")\n",
    "else:\n",
    "    print(\"No data available for FFT analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e3bf4",
   "metadata": {},
   "source": [
    "## 19. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f446a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 90)\n",
    "print(\"DATA EXPLORATION AND ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"   Total Experiments: {len(experiment_folders)}\")\n",
    "print(f\"   Seismic Datasets Loaded: {len(seismic_data)}\")\n",
    "print(f\"   Acoustic Datasets Loaded: {len(acoustic_data)}\")\n",
    "print(f\"   GPS Datasets Loaded: {len(gps_data)}\")\n",
    "\n",
    "print(\"\\n2. VEHICLE TYPES\")\n",
    "print(\"-\" * 90)\n",
    "vehicles = set()\n",
    "for exp in experiment_folders:\n",
    "    if 'Polaris' in exp:\n",
    "        vehicles.add('Polaris')\n",
    "    elif 'Silverado' in exp:\n",
    "        vehicles.add('Silverado')\n",
    "    elif 'Warhog' in exp:\n",
    "        vehicles.add('Warhog')\n",
    "print(f\"   Vehicles: {', '.join(sorted(vehicles))}\")\n",
    "\n",
    "print(\"\\n3. DATA COLLECTION CONDITIONS\")\n",
    "print(\"-\" * 90)\n",
    "los_count = sum(1 for exp in experiment_folders if 'NoLineOfSight' not in exp)\n",
    "no_los_count = sum(1 for exp in experiment_folders if 'NoLineOfSight' in exp)\n",
    "print(f\"   Line-of-Sight: {los_count} experiments\")\n",
    "print(f\"   No Line-of-Sight: {no_los_count} experiments\")\n",
    "\n",
    "print(\"\\n4. DATA QUALITY\")\n",
    "print(\"-\" * 90)\n",
    "total_seismic_samples = sum(len(df) for df in seismic_data.values())\n",
    "total_acoustic_samples = sum(len(df) for df in acoustic_data.values())\n",
    "print(f\"   Total Seismic Samples: {total_seismic_samples:,}\")\n",
    "print(f\"   Total Acoustic Samples: {total_acoustic_samples:,}\")\n",
    "print(f\"   Missing Values: Checked (see Data Quality Assessment section)\")\n",
    "\n",
    "print(\"\\n5. SIGNAL CHARACTERISTICS\")\n",
    "print(\"-\" * 90)\n",
    "if len(seismic_stats) > 0:\n",
    "    avg_seismic_rms = seismic_stats_df['RMS'].mean()\n",
    "    print(f\"   Average Seismic RMS: {avg_seismic_rms:.4e}\")\n",
    "if len(acoustic_stats) > 0:\n",
    "    avg_acoustic_rms = acoustic_stats_df['RMS'].mean()\n",
    "    print(f\"   Average Acoustic RMS: {avg_acoustic_rms:.4e}\")\n",
    "\n",
    "print(\"\\n6. KEY FINDINGS\")\n",
    "print(\"-\" * 90)\n",
    "print(\"   âœ“ Time series visualizations show signal patterns for all experiments\")\n",
    "print(\"   âœ“ Distribution analysis reveals signal amplitude characteristics\")\n",
    "print(\"   âœ“ Vehicle type comparison completed\")\n",
    "print(\"   âœ“ Line-of-sight vs No line-of-sight comparison completed\")\n",
    "print(\"   âœ“ GPS trajectories mapped (where available)\")\n",
    "print(\"   âœ“ Frequency domain analysis performed\")\n",
    "\n",
    "if len(correlation_data) > 0:\n",
    "    print(f\"   âœ“ Acoustic-Seismic correlation: {correlation:.4f}\")\n",
    "\n",
    "print(\"\\n7. RECOMMENDATIONS\")\n",
    "print(\"-\" * 90)\n",
    "print(\"   â€¢ Further analysis of frequency domain patterns\")\n",
    "print(\"   â€¢ Machine learning model development for vehicle detection\")\n",
    "print(\"   â€¢ Cross-sensor fusion techniques\")\n",
    "print(\"   â€¢ Time-synchronized analysis between acoustic and seismic signals\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fdfcd",
   "metadata": {},
   "source": [
    "## 20. Cleaned Data Analysis - Key Insights and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687fef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"CLEANED DATA ANALYSIS - KEY INSIGHTS AND FINDINGS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\nðŸ“Š 1. DATA CLEANING IMPACT\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\\nSeismic Data Cleaning:\")\n",
    "print(\"   â€¢ DC Offset Removed: ~16,280 ADC counts (biased all raw measurements)\")\n",
    "print(\"   â€¢ Effect: Raw data showed all vehicles with similar RMS (~16,280)\")\n",
    "print(\"   â€¢ Cleaned data reveals 3x differences between vehicles\")\n",
    "print(\"   â€¢ Outliers Removed: 0.18% - 3.56% of samples per experiment\")\n",
    "print(\"   â€¢ Method: Subtract mean + IQR-based outlier removal (3Ã—IQR threshold)\")\n",
    "\n",
    "print(\"\\nAcoustic Data Cleaning:\")\n",
    "print(\"   â€¢ Outliers Removed: 2.76% - 12.82% of samples per experiment\")\n",
    "print(\"   â€¢ Method: IQR-based detection with median replacement\")\n",
    "print(\"   â€¢ Result: Reduced noise and improved signal clarity\")\n",
    "\n",
    "print(\"\\nðŸš— 2. VEHICLE SIGNATURE COMPARISON (Cleaned Data)\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate vehicle statistics\n",
    "vehicle_seismic = cleaned_df.groupby('Vehicle')['Seismic RMS'].agg(['mean', 'std', 'min', 'max'])\n",
    "vehicle_acoustic = cleaned_df.groupby('Vehicle')['Acoustic RMS'].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "print(\"\\nSeismic Signal Strength by Vehicle:\")\n",
    "for vehicle in vehicle_seismic.index:\n",
    "    mean_val = vehicle_seismic.loc[vehicle, 'mean']\n",
    "    std_val = vehicle_seismic.loc[vehicle, 'std']\n",
    "    min_val = vehicle_seismic.loc[vehicle, 'min']\n",
    "    max_val = vehicle_seismic.loc[vehicle, 'max']\n",
    "    print(f\"   â€¢ {vehicle:12s}: RMS = {mean_val:7.1f} Â± {std_val:5.1f}  (range: {min_val:6.1f} - {max_val:7.1f})\")\n",
    "\n",
    "print(\"\\nAcoustic Signal Strength by Vehicle:\")\n",
    "for vehicle in vehicle_acoustic.index:\n",
    "    mean_val = vehicle_acoustic.loc[vehicle, 'mean']\n",
    "    std_val = vehicle_acoustic.loc[vehicle, 'std']\n",
    "    min_val = vehicle_acoustic.loc[vehicle, 'min']\n",
    "    max_val = vehicle_acoustic.loc[vehicle, 'max']\n",
    "    print(f\"   â€¢ {vehicle:12s}: RMS = {mean_val:7.1f} Â± {std_val:5.1f}  (range: {min_val:6.1f} - {max_val:7.1f})\")\n",
    "\n",
    "# Calculate vehicle ranking\n",
    "seismic_ranking = vehicle_seismic['mean'].sort_values(ascending=False)\n",
    "acoustic_ranking = vehicle_acoustic['mean'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nðŸ† VEHICLE SIGNATURE STRENGTH RANKING:\")\n",
    "print(\"\\nSeismic Signature (Strongest to Weakest):\")\n",
    "for i, (vehicle, rms) in enumerate(seismic_ranking.items(), 1):\n",
    "    print(f\"   {i}. {vehicle:12s}: {rms:7.1f} RMS\")\n",
    "    \n",
    "print(\"\\nAcoustic Signature (Strongest to Weakest):\")\n",
    "for i, (vehicle, rms) in enumerate(acoustic_ranking.items(), 1):\n",
    "    print(f\"   {i}. {vehicle:12s}: {rms:7.1f} RMS\")\n",
    "\n",
    "print(\"\\nðŸ” 3. LINE-OF-SIGHT IMPACT ANALYSIS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate condition statistics\n",
    "condition_seismic = cleaned_df.groupby('Condition')['Seismic RMS'].agg(['mean', 'std', 'count'])\n",
    "condition_acoustic = cleaned_df.groupby('Condition')['Acoustic RMS'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"\\nSeismic Signal by Line-of-Sight Condition:\")\n",
    "for condition in condition_seismic.index:\n",
    "    mean_val = condition_seismic.loc[condition, 'mean']\n",
    "    std_val = condition_seismic.loc[condition, 'std']\n",
    "    count = int(condition_seismic.loc[condition, 'count'])\n",
    "    print(f\"   â€¢ {condition:10s}: RMS = {mean_val:7.1f} Â± {std_val:5.1f}  (n={count} experiments)\")\n",
    "\n",
    "print(\"\\nAcoustic Signal by Line-of-Sight Condition:\")\n",
    "for condition in condition_acoustic.index:\n",
    "    mean_val = condition_acoustic.loc[condition, 'mean']\n",
    "    std_val = condition_acoustic.loc[condition, 'std']\n",
    "    count = int(condition_acoustic.loc[condition, 'count'])\n",
    "    print(f\"   â€¢ {condition:10s}: RMS = {mean_val:7.1f} Â± {std_val:5.1f}  (n={count} experiments)\")\n",
    "\n",
    "# Calculate percentage difference\n",
    "if 'LOS' in condition_seismic.index and 'No LOS' in condition_seismic.index:\n",
    "    seismic_diff_pct = ((condition_seismic.loc['No LOS', 'mean'] - condition_seismic.loc['LOS', 'mean']) / \n",
    "                        condition_seismic.loc['LOS', 'mean'] * 100)\n",
    "    print(f\"\\nSeismic Signal Difference (No LOS vs LOS): {seismic_diff_pct:+.1f}%\")\n",
    "    \n",
    "if 'LOS' in condition_acoustic.index and 'No LOS' in condition_acoustic.index:\n",
    "    acoustic_diff_pct = ((condition_acoustic.loc['No LOS', 'mean'] - condition_acoustic.loc['LOS', 'mean']) / \n",
    "                         condition_acoustic.loc['LOS', 'mean'] * 100)\n",
    "    print(f\"Acoustic Signal Difference (No LOS vs LOS): {acoustic_diff_pct:+.1f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ 4. KEY FINDINGS AND INSIGHTS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Calculate key metrics\n",
    "silverado_seismic = vehicle_seismic.loc['Silverado', 'mean']\n",
    "warhog_seismic = vehicle_seismic.loc['Warhog', 'mean']\n",
    "ratio_seismic = silverado_seismic / warhog_seismic\n",
    "\n",
    "warhog_acoustic = vehicle_acoustic.loc['Warhog', 'mean']\n",
    "polaris_acoustic = vehicle_acoustic.loc['Polaris', 'mean']\n",
    "ratio_acoustic = warhog_acoustic / polaris_acoustic\n",
    "\n",
    "print(\"\\nâœ“ CRITICAL DISCOVERY: DC Offset Masking\")\n",
    "print(f\"   Raw seismic data had DC offset of ~16,280 ADC counts that masked all vehicle differences\")\n",
    "print(f\"   After removal, Silverado shows {ratio_seismic:.1f}x stronger seismic signature than Warhog\")\n",
    "\n",
    "print(\"\\nâœ“ VEHICLE DISTINGUISHABILITY:\")\n",
    "print(f\"   â€¢ Seismic: Silverado (1,216.3) >> Polaris (450.7) > Warhog (389.6)\")\n",
    "print(f\"   â€¢ Acoustic: Warhog (726.6) > Silverado (687.7) > Polaris (579.5)\")\n",
    "print(f\"   â€¢ Different vehicles produce distinct signatures in both sensor types\")\n",
    "print(f\"   â€¢ Heavier vehicles (Silverado) generate stronger seismic signals\")\n",
    "print(f\"   â€¢ Engine/exhaust characteristics dominate acoustic signatures\")\n",
    "\n",
    "print(\"\\nâœ“ MULTI-MODAL SENSOR FUSION:\")\n",
    "print(f\"   â€¢ Seismic and acoustic rankings differ significantly\")\n",
    "print(f\"   â€¢ Combining both modalities provides better vehicle classification\")\n",
    "print(f\"   â€¢ Warhog: low seismic but high acoustic (engine noise dominant)\")\n",
    "print(f\"   â€¢ Silverado: high seismic (weight) and high acoustic (powerful engine)\")\n",
    "print(f\"   â€¢ Polaris: moderate seismic and lowest acoustic (lighter, quieter)\")\n",
    "\n",
    "print(\"\\nâœ“ LINE-OF-SIGHT EFFECTS:\")\n",
    "if 'LOS' in condition_seismic.index and 'No LOS' in condition_seismic.index:\n",
    "    if abs(seismic_diff_pct) < 10:\n",
    "        print(f\"   â€¢ Seismic signals relatively unaffected by LOS ({seismic_diff_pct:+.1f}% difference)\")\n",
    "        print(f\"   â€¢ Ground-coupled vibrations propagate regardless of visual obstruction\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Seismic signals show {abs(seismic_diff_pct):.1f}% difference between conditions\")\n",
    "        \n",
    "if 'LOS' in condition_acoustic.index and 'No LOS' in condition_acoustic.index:\n",
    "    if abs(acoustic_diff_pct) < 10:\n",
    "        print(f\"   â€¢ Acoustic signals relatively stable ({acoustic_diff_pct:+.1f}% difference)\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Acoustic signals show {abs(acoustic_diff_pct):.1f}% difference between conditions\")\n",
    "        print(f\"   â€¢ Terrain/obstacles may affect acoustic propagation\")\n",
    "\n",
    "print(\"\\nâœ“ DATA QUALITY IMPROVEMENTS:\")\n",
    "print(f\"   â€¢ Cleaned seismic data centered around 0 (mean = -0.73 to 3.26)\")\n",
    "print(f\"   â€¢ Standard deviations now reflect true signal variability\")\n",
    "print(f\"   â€¢ Peak detection more reliable after outlier removal\")\n",
    "print(f\"   â€¢ Vehicle signatures clearly separated for classification\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ 5. RECOMMENDED NEXT STEPS\")\n",
    "print(\"-\" * 100)\n",
    "print(\"\\n   1. MACHINE LEARNING CLASSIFICATION:\")\n",
    "print(\"      â€¢ Build multi-class classifier using cleaned seismic + acoustic features\")\n",
    "print(\"      â€¢ Expected accuracy: >90% based on clear signal separation\")\n",
    "print(\"      â€¢ Features: RMS, std dev, peak amplitude, frequency domain\")\n",
    "\n",
    "print(\"\\n   2. FEATURE ENGINEERING:\")\n",
    "print(\"      â€¢ Extract time-domain features (zero-crossings, energy)\")\n",
    "print(\"      â€¢ Frequency domain features (dominant frequencies, spectral centroid)\")\n",
    "print(\"      â€¢ Sensor fusion features (seismic/acoustic ratio, cross-correlation)\")\n",
    "\n",
    "print(\"\\n   3. REAL-TIME DETECTION:\")\n",
    "print(\"      â€¢ Implement sliding window analysis for continuous monitoring\")\n",
    "print(\"      â€¢ Set vehicle-specific detection thresholds based on cleaned data\")\n",
    "print(\"      â€¢ Use multi-sensor fusion for robust classification\")\n",
    "\n",
    "print(\"\\n   4. ENVIRONMENTAL FACTORS:\")\n",
    "print(\"      â€¢ Analyze speed vs signal strength relationships (GPS data)\")\n",
    "print(\"      â€¢ Study terrain effects on signal propagation\")\n",
    "print(\"      â€¢ Model distance-based signal attenuation\")\n",
    "\n",
    "print(\"\\n   5. VALIDATION:\")\n",
    "print(\"      â€¢ Test classifier on additional experiments\")\n",
    "print(\"      â€¢ Evaluate performance under different environmental conditions\")\n",
    "print(\"      â€¢ Assess false positive/negative rates for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"END OF ANALYSIS\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
